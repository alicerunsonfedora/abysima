{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Abysima: Language Generation Experiments\n",
    "\n",
    "The following notebook will experiment with generating a language using neural networks and generative deep learning.\n",
    "This is, by no means, a production-ready system, nor is it a complete network; rather, the purpose of this experiment\n",
    "is to see what is possible with creating a language.\n",
    "\n",
    "For more information on the process and supporting research, please refer to the Linguistics Paper document found in the\n",
    "`01 - Areas of Responsibility` directory.\n",
    "\n",
    "The following source code and datasets are licensed under the Mozilla Public License v2.0. Please refer to the LICENSE\n",
    "file that came with this repository for more information on what your rights are with usage and modification of this\n",
    "software. If a LICENSE file is not provided, you can obtain a copy at https://www.mozilla.org/en-US/MPL/2.0/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collecting the Dataset\n",
    "\n",
    "Before we can create networks that will be able to train off of words, sniglets, and non-words, we first need to create\n",
    "a dataset that the networking models will be able to understand.\n",
    "\n",
    "To do this, we will create two pools of data: a list full of valid words, and a list of random strings, both of equal\n",
    "count. The list of valid words derives from the words found in the `/usr/share/dict/words` file found on UNIX and Linux\n",
    "systems, which contains words of various languages. A list of basic Japanese words written in Romaji are also included\n",
    "in the list.\n",
    "\n",
    "Words in the valid pool are then trimmed based on the average length of the words in that dataset, removing any words\n",
    "that are bigger than the specified length. This is crucial because this will help prevent miscalculations due to a lot\n",
    "of whitespace. Additionally, words that are acronyms, contractions, and/or less than three characters long are removed\n",
    "from the list. The invalid word dataset is generated from an algorithm that randomly selects letters from three to the\n",
    "average length.\n",
    "\n",
    "Neural networks need each entry in the dataset to be of the same length, since they are fundamentally rows in a large\n",
    "matrix. To accomplish this, we will append space characters (`' '`) at the end of words when needed; this is known as\n",
    "_padding a sequence_.\n",
    "\n",
    "We also need to be able to indicate which words are valid and which ones were randomly generated. To do this, we will\n",
    "add an extra column to the dataset that will indicate its validity by writing either \"valid\" or \"invalid\".\n",
    "\n",
    "The dataset is then shuffled between twenty-five to fifty times to make sure that we aren't only training on valid words\n",
    "or vice-versa. Once this shuffling is complete, we will split the dataset into two pools, where eighty percent (80%) of\n",
    "the data will go into a training pool, and twenty percent (20%) will go into a testing pool. The training pool will be\n",
    "used to train the network, and the testing pool will be used to test that the trained network is making as close to an\n",
    "accurate prediction as possible.\n",
    "\n",
    "We then take these two pools and write them to CSV files which all of our models will be able to read. The script that\n",
    "implements this process is available in `create_dataset.py` in the project's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pandas library, which will read the CSV files that we wrote.\n",
    "import pandas as pd\n",
    "\n",
    "# Import the training and testing pools.\n",
    "DF_TRAINING_POOL = pd.read_csv(\"../datasets/dtrain.csv\")\n",
    "DF_TESTING_POOL = pd.read_csv(\"../datasets/dtest.csv\")\n",
    "\n",
    "# Make a preview of the data frame from the training pool. Note that our features are the eight characters, and the\n",
    "# target is the 'Valid' column.\n",
    "DF_TRAINING_POOL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (79572, 9)\n",
      "Test shape: (19894, 9)\n",
      "Total rows: 99466\n"
     ]
    }
   ],
   "source": [
    "# Neural networks will need to encode the data in order to be able to train. We will write an encoder and apply it to\n",
    "# the dataset here.\n",
    "from string import ascii_lowercase\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def encode_features(feature) -> float:\n",
    "    \"\"\"Returns an encoded number that represents the data item.\"\"\"\n",
    "    if feature == ' ' or feature == 'invalid':\n",
    "        return 0.0\n",
    "    elif feature == 'valid':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (ascii_lowercase.index(feature) + 1) / 26.0\n",
    "\n",
    "\n",
    "# Convert the data frames into NumPy arrays, which will be used in the neural networks.\n",
    "DATA_TRAIN = DF_TRAINING_POOL.to_numpy()\n",
    "DATA_TEST = DF_TESTING_POOL.to_numpy()\n",
    "\n",
    "# Make the mapping function that will convert the strings in the datasets into numbers with the function we defined\n",
    "# earlier and map them on our datasets.\n",
    "map_func = np.vectorize(encode_features)\n",
    "DATA_TRAIN = map_func(DATA_TRAIN)\n",
    "DATA_TEST = map_func(DATA_TEST)\n",
    "\n",
    "DATASET_COUNT = DATA_TRAIN.shape[0] + DATA_TEST.shape[0]  # type: ignore\n",
    "\n",
    "# Print out the sizes of the training and testing datasets.\n",
    "print(f\"Train shape: {DATA_TRAIN.shape}\")  # type: ignore\n",
    "print(f\"Test shape: {DATA_TEST.shape}\")  # type: ignore\n",
    "print(f\"Total rows: {DATASET_COUNT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training and testing datasets into X and Y components. X will contain all of the features, and Y will\n",
    "# contain the target value.\n",
    "X_train, y_train = DATA_TRAIN[:, :-1], DATA_TRAIN[:, -1]  # type: ignore\n",
    "X_test, y_test = DATA_TEST[:, :-1], DATA_TEST[:, -1]  # type: ignore\n",
    "\n",
    "print(X_train[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating our Networks\n",
    "\n",
    "Now that we have our dataset ready, we will begin creating neural networks that will train on the data we specify.\n",
    "These neural networks operate similar to our own brains and will try to \"learn\" what makes a word valid by using\n",
    "mathematical equations running in the background.\n",
    "\n",
    "To accomplish this, we will utilize two frameworks that exist: Tensorflow and CoreML. Tensorflow is a library created\n",
    "by Google to make neural networks from scratch without writing all of the code to process the math. Likewise, CoreML\n",
    "is a library made by Apple that lets developers create neural networks for use in apps on their platforms (macOS, iOS,\n",
    "tvOS, and watchOS).\n",
    "\n",
    "For this experiment, we will design three networks:\n",
    "\n",
    "- First, a fully-connected neural network (FCNN). This type of network indicates that all of the nodes in the network\n",
    "  link up to each other in some way. This is the most \"basic\" neural network in the list.\n",
    "- Next, a recurrent neural network (RNN) using the Long Short-Term Memory strategy (LSTM). Recurrent neural networks\n",
    "  operate very similarly to FCNNs in that nodes are connected. However, it recognizes that the data it receives is\n",
    "  sequential, meaning that they appear in a sequence. The network will perform mathematical operations and learn with\n",
    "  this in mind.\n",
    "- Finally, a CoreML model created with Create ML. This dataset is pre-trained and automatically selected an algorithm\n",
    "  that it thinks works best for the dataset. Exact implementation is unknown since Apple hides this from the developer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will specify parameters here that will be used to train the networks we are creating. These parameters can be\n",
    "# adjusted by us at any time to optimize the algorithms. These parameters are known as 'hyperparameters'.\n",
    "\n",
    "# Specify the number of \"iterations\" the neural networks will run under. In this case, an iteration indicates a session\n",
    "# of training by reading the data and running operations on it.\n",
    "KERAS_EPOCHS = 500\n",
    "\n",
    "# Specify the number of batches the neural networks will use. To speed up training, our networks will run updates after\n",
    "# a certain number of batches, making updates as necessary.\n",
    "KERAS_BATCHES = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Tensorflow and Keras libraries needed to make two of the networks.\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Create the FCNN. This will have a first layer that maps to the number of characters in our set: in this case, 8. We\n",
    "# also include some hidden layers of various lengths before including a final layer that will filter down to a single\n",
    "# input.\n",
    "FCNN = keras.Sequential()\n",
    "FCNN.add(Dense(8, input_dim=8, activation=\"relu\"))\n",
    "FCNN.add(Dense(32, activation='relu'))\n",
    "FCNN.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model and use 'binary cross-entropy' to foce the network to either say \"yes\" or \"no\". We will also use the\n",
    "# adam optimizer and list accuracy in our metrics for further analysis.\n",
    "FCNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print out a summary of the FCNN.\n",
    "FCNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now begin training the FCNN with the training data we encoded earlier, using the hyperparameters we defined.\n",
    "# To prevent overcorrection or memorization of the data, we will dedicate 20% of the data to validation. We will store\n",
    "# the results of the training session for later analysis.\n",
    "FCNN_results = FCNN.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=KERAS_EPOCHS,\n",
    "                        batch_size=KERAS_BATCHES,\n",
    "                        verbose=1,\n",
    "                        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create the recurrent neural network in a similar fashion.\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "# Create the FCNN. This will have a first layer that maps to the number of characters in our set: in this case, 8. We\n",
    "# also include some hidden layers of various lengths before including a final layer that will filter down to a single\n",
    "# input.\n",
    "RNN = keras.Sequential()\n",
    "RNN.add(Embedding(DATASET_COUNT + 1, 64, input_length=8))\n",
    "RNN.add(LSTM(4, activation='tanh'))\n",
    "RNN.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model and use 'binary cross-entropy' to foce the network to either say \"yes\" or \"no\". We will also use the\n",
    "# adam optimizer and list accuracy in our metrics for further analysis.\n",
    "RNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print out a summary of the FCNN.\n",
    "RNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now begin training the RNN with the training data we encoded earlier, using the hyperparameters we defined.\n",
    "# To prevent overcorrection or memorization of the data, we will dedicate 20% of the data to validation. We will store\n",
    "# the results of the training session for later analysis.\n",
    "RNN_results = RNN.fit(X_train,\n",
    "                      y_train,\n",
    "                      epochs=KERAS_EPOCHS,\n",
    "                      batch_size=KERAS_BATCHES,\n",
    "                      verbose=1,\n",
    "                      validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae728774eee256562aff2651c76309c3916f29437b55f618701e1f4834ebef1b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('env_tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
